import json
from langchain_openai import ChatOpenAI
from retriever import retrieve_across_all

# ==========================================================
#  Translator (KOR â†” ENG)
# ==========================================================
translator = ChatOpenAI(model="gpt-4o-mini", temperature=0)

def translate_to_english(query):
    prompt = f"""
Translate this into FIA Sporting Regulations style English.
Do NOT simplify terms. Maintain technical vocabulary.

Query:
{query}
"""
    return translator.invoke(prompt).content.strip()

def translate_to_korean(text):
    prompt = f"""
ì•„ë˜ ì˜ë¬¸ ë‚´ìš©ì„ FIA ê¸°ìˆ /ìŠ¤í¬íŒ… ê·œì • ë¬¸ì²´ì— ë§ê²Œ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­í•˜ì„¸ìš”.
ìˆ«ì, ë‹¨ì–´, ìš©ì–´ëŠ” ì›ë¬¸ì„ ì •í™•í•˜ê²Œ ìœ ì§€í•˜ì„¸ìš”.

í…ìŠ¤íŠ¸:
{text}
"""
    return translator.invoke(prompt).content.strip()


# ==========================================================
#  ì¤‘ë³µ ì œê±° ë¡œì§ (storeê°€ ë‹¬ë¼ë„ ê°™ì€ chunkëŠ” ì œê±°)
# ==========================================================
def dedupe_docs(docs):
    seen = set()
    unique = []
    for d in docs:
        sig = d.page_content.strip()[:200]   # ì• 200ì ê¸°ì¤€ signature
        if sig not in seen:
            seen.add(sig)
            unique.append(d)
    return unique


# ==========================================================
#  Relevance Scoring (Text only)
# ==========================================================
def relevance_score(doc, query):
    if doc.metadata.get("type") == "table":
        return 0
    text = doc.page_content.lower()
    q_words = query.lower().split()
    return sum(1 for w in q_words if w in text)


# ==========================================================
#  Table Parsing
# ==========================================================
def parse_table_json(doc):
    try:
        return json.loads(doc.page_content)
    except:
        return None


# ==========================================================
#  ê·œì • ë¬¸ì¥ ìŠ¤íƒ€ì¼ëŸ¬ (Streamlit-safe)
# ==========================================================
def style_regulation_sentence(text, citation):
    return f"""
<div style="color:#1a73e8; font-weight:500; margin-top:4px;">
{text}
</div>
<div style="color:#666; font-size:0.8em; margin-bottom:8px;">
ğŸ“ {citation}
</div>
"""


# ==========================================================
#  UI-Friendly Formatter
# ==========================================================
def format_output(main_answer, regulation_blocks):
    """
    regulation_blocks = [
        {"text": "...", "citation": "..."},
        ...
    ]
    """
    html = f"""
### ğŸ“˜ ë‹µë³€
{main_answer}
<br/><br/>
"""

    if regulation_blocks:
        html += "### ğŸ“ ê·œì • ì¸ìš©<br/>"
        for blk in regulation_blocks:
            html += style_regulation_sentence(blk["text"], blk["citation"])

    return html


# ==========================================================
#                     MAIN RAG Q&A
# ==========================================================
def ask_question(query: str, k: int = 8):

    llm = ChatOpenAI(model="gpt-4o", temperature=0)

    # ------------------------------------------------------
    #  1) Query EN ë³€í™˜
    # ------------------------------------------------------
    query_en = translate_to_english(query)

    # ------------------------------------------------------
    #  2) í•œêµ­ì–´ + ì˜ì–´ ê²€ìƒ‰ â†’ ê²°ê³¼ ë³‘í•© í›„ ì¤‘ë³µ ì œê±°
    # ------------------------------------------------------
    docs_ko = retrieve_across_all(query, k=k)
    docs_en = retrieve_across_all(query_en, k=k)

    docs = dedupe_docs(docs_ko + docs_en)

    if not docs:
        return format_output("ê²€ìƒ‰ëœ ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤.", [])

    # ------------------------------------------------------
    #  3) ë¬¸ì„œ ë¶„ë¦¬
    # ------------------------------------------------------
    text_docs = [d for d in docs if d.metadata.get("type") != "table"]
    table_docs = [d for d in docs if d.metadata.get("type") == "table"]

    # relevance sorting
    text_docs = sorted(text_docs, key=lambda d: relevance_score(d, query_en), reverse=True)
    text_docs = text_docs[:3]
    table_docs = table_docs[:2]

    # ------------------------------------------------------
    #  4) Context êµ¬ì„±
    # ------------------------------------------------------
    context_blocks = []
    citation_raw = []

    for d in text_docs:
        context_blocks.append(d.page_content)
        citation_raw.append({
            "text": d.page_content[:300].replace("\n", " "),
            "citation": f"{d.metadata.get('source_store')} Â· p.{d.metadata.get('page')}"
        })

    for d in table_docs:
        table_data = parse_table_json(d)
        if table_data:
            context_blocks.append("TABLE_DATA:\n" + json.dumps(table_data, indent=2))
            citation_raw.append({
                "text": str(table_data),
                "citation": f"{d.metadata.get('source_store')} Â· p.{d.metadata.get('page')}"
            })

    context = "\n\n".join(context_blocks)

    # ------------------------------------------------------
    #  5) Overlap íŒë‹¨
    # ------------------------------------------------------
    q_words = query_en.lower().split()
    overlap = sum(1 for w in q_words if w in context.lower())

    # ------------------------------------------------------
    #  6) Fallback â€” ë¬¸ì„œ ê¸°ë°˜ ë‚´ìš© ì—†ìŒ
    # ------------------------------------------------------
    if overlap == 0:
        prompt = f"""
You are an F1 expert. The question does not appear in the regulations.

Provide ONLY commonly-known F1 knowledge.
Do not invent article numbers or regulations.

Question:
{query}

Answer:
"""
        raw = llm.invoke(prompt).content.strip()
        answer_ko = translate_to_korean(raw)

        return format_output(answer_ko, [])

    # ------------------------------------------------------
    #  7) ë¬¸ì„œ ê¸°ë°˜ RAG ë‹µë³€
    # ------------------------------------------------------
    prompt = f"""
You are an FIA Sporting Regulations expert.

Use ONLY information appearing in Context.
If sentences are duplicated in Context, summarize them once.

[Context]
{context}

[Question]
{query}

[Answer]
"""
    raw_answer_en = llm.invoke(prompt).content.strip()
    answer_ko = translate_to_korean(raw_answer_en)

    # ------------------------------------------------------
    #  8) ê·œì • ì¸ìš© (ì¤‘ë³µ ì œê±°)
    # ------------------------------------------------------
    seen = set()
    reg_blocks = []
    for c in citation_raw:
        key = c["text"][:150]
        if key not in seen:
            seen.add(key)
            reg_blocks.append(c)

    return format_output(answer_ko, reg_blocks)
