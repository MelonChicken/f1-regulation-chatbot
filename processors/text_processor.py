import re
import os

from langchain_community.document_loaders import PyPDFLoader
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma


# -----------------------------------------------------------
# 1. PDF ë¡œë“œ
# -----------------------------------------------------------
def load_pdf(path):
    loader = PyPDFLoader(path)
    return loader.load()


# -----------------------------------------------------------
# 2. ARTICLE split
# -----------------------------------------------------------
def split_by_article(pages):
    combined = "".join(p.page_content + "\n" for p in pages)

    pattern = r"(ARTICLE\s+B\d+(?::)?[^\n]*)"
    parts = re.split(pattern, combined)

    article_chunks = []

    for i in range(1, len(parts), 2):
        title = parts[i].strip()
        body = parts[i + 1].strip()

        # ğŸ”¥ bodyê°€ ì¶©ë¶„íˆ ê¸¸ì§€ ì•Šìœ¼ë©´ skip
        if len(body) < 15:
            continue

        article_chunks.append((title, body))

    return article_chunks


# -----------------------------------------------------------
# 3. SECTION split
# -----------------------------------------------------------
def split_into_sections(article_title, body_text):
    pattern = r"\b(B\d+(?:\.\d+)+)\b"
    parts = re.split(pattern, body_text)

    sections = []

    intro = parts[0].strip()
    if len(intro) > 5:
        sections.append(
            Document(
                page_content=intro,
                metadata={"article": article_title, "section": "intro"}
            )
        )

    for i in range(1, len(parts), 2):
        section_id = parts[i].strip()
        text = parts[i + 1].strip()

        # ğŸ”¥ ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ skip
        if len(text) < 5:
            continue

        sections.append(
            Document(
                page_content=text,
                metadata={"article": article_title, "section": section_id}
            )
        )

    return sections


# -----------------------------------------------------------
# 4. ìµœì í™” Chunking
# -----------------------------------------------------------

def chunk_optimize(sections, max_chars: int = 1000, overlap: int = 200):
    """
    Section ë‹¨ìœ„ chunking ì „ëµ:

    - ê¸°ë³¸ ë‹¨ìœ„ëŠ” Section í•˜ë‚˜ (B1.7.3 ì „ì²´ë¥¼ í•˜ë‚˜ë¡œ ìœ ì§€)
    - Section í…ìŠ¤íŠ¸ ê¸¸ì´ê°€ max_chars ì´í•˜ì´ë©´ ê·¸ëŒ€ë¡œ í•œ ê°œ chunkë¡œ ì‚¬ìš©
    - ë„ˆë¬´ ê¸´ Sectionë§Œ RecursiveCharacterTextSplitterë¡œ ë‚˜ëˆ”
    """

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=max_chars,
        chunk_overlap=overlap,
        separators=["\n\n", "\n", ". ", " "],
    )

    optimized_chunks = []

    for sec in sections:
        text = (sec.page_content or "").strip()
        if not text:
            continue

        # ì„¹ì…˜ ì „ì²´ ê¸¸ì´ê°€ ì§§ìœ¼ë©´ ê·¸ëƒ¥ í•œ ë©ì–´ë¦¬ë¡œ ì‚¬ìš©
        if len(text) <= max_chars:
            optimized_chunks.append(
                Document(
                    page_content=text,
                    metadata={
                        "article": sec.metadata.get("article"),
                        "section": sec.metadata.get("section"),
                        "subchunk_index": 0,
                    },
                )
            )
            continue

        # ë„ˆë¬´ ê¸´ ì„¹ì…˜ë§Œ splitterë¡œ ì¬ë¶„í• 
        split_texts = splitter.split_text(text)

        for i, chunk in enumerate(split_texts):
            chunk = chunk.strip()
            if not chunk:
                continue

            optimized_chunks.append(
                Document(
                    page_content=chunk,
                    metadata={
                        "article": sec.metadata.get("article"),
                        "section": sec.metadata.get("section"),
                        "subchunk_index": i,
                    },
                )
            )

    return optimized_chunks



# -----------------------------------------------------------
# 5. Chroma ì €ì¥
# -----------------------------------------------------------
def save_vectorstore(chunks, persist_dir):
    embeddings = OpenAIEmbeddings(model="text-embedding-3-large")
    os.makedirs(persist_dir, exist_ok=True)

    # â­ ìµœì¢… í•„í„°ë§ (100% ë³´í˜¸)
    clean_chunks = [
        c for c in chunks
        if c.page_content and c.page_content.strip()
    ]

    if len(clean_chunks) == 0:
        raise ValueError(f"No valid chunks found to embed for {persist_dir}")

    return Chroma.from_documents(
        documents=clean_chunks,
        embedding=embeddings,
        persist_directory=persist_dir
    )
def fallback_chunking(pages):
    """
    ARTICLE íŒ¨í„´ì´ ì „í˜€ ì—†ëŠ” ê·œì • ë¬¸ì„œë¥¼ ìœ„í•œ fallback chunking
    - ì „ì²´ ë¬¸ì„œë¥¼ ê·¸ëŒ€ë¡œ chunking
    - Technical Regulations, Appendix, Annex ë“± ì²˜ë¦¬ ê°€ëŠ¥
    """
    raw_text = "".join(p.page_content + "\n" for p in pages)

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", ". ", " "]
    )

    chunks = splitter.split_text(raw_text)

    documents = []
    for idx, c in enumerate(chunks):
        if c.strip():
            documents.append(
                Document(
                    page_content=c,
                    metadata={
                        "article": "unknown",
                        "section": "fallback",
                        "subchunk_index": idx
                    }
                )
            )
    return documents
